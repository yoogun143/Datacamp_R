# Determine row to split on: split (80:20)
split <- round(nrow(diamonds) * .8)
# Create train
train <- diamonds[1:split,]
# Create test
test <- diamonds[(split + 1): nrow(diamonds), ]
# Fit lm model on train: model
model <- lm(price ~ ., data = train)
# Predict on test: p
p <- predict(model, test)
# Compute errors: error
error <- p - test$price
# Calculate RMSE
sqrt(mean(error^2))
install.packages("mlbench")
library(mlbench)
data("Sonar")
#### 60/40 SPLIT
# Shuffle row indices: rows
rows <- sample(nrow(Sonar))
# Randomly order data: Sonar
Sonar <- Sonar[rows, ]
# Identify row to split on: split
split <- round(nrow(Sonar) * .6)
# Create train
train <- Sonar[1:split, ]
# Create test
test <- Sonar[(split + 1):nrow(Sonar), ]
#### FIT LOGIT MODEL
# Fit glm model: model
model <- glm(Class ~ ., family = "binomial", train)
# Predict on test: p
p <- predict(model, test, type = "response")
glm()
?train
#### FIT LOGIT MODEL
# Fit glm model: model
model <- glm(Class ~ ., family = binomial(link = "logit"), train)
# Predict on test: p
p <- predict(model, test, type = "response")
#### CONFUSION MATRIX
# Calculate class probabilities: p_class
p_class <- ifelse(p >  0.5, "M", "R")
# Create confusion matrix
confusionMatrix(p_class, test$Class)
16/(16+22)
16/(16+29)
# Apply threshold of 0.9: p_class
p_class <- ifelse(p > 0.9, "M", "R")
# Create confusion matrix
confusionMatrix(p_class, test$Class)
# Apply threshold of 0.10: p_class
p_class <- ifelse(p > 0.1, "M", "R")
# Create confusion matrix
confusionMatrix(p_class, test$Class)
#### ROC CURVE
# Predict on test: p
p <- predict(model, test, type = "response")
# Make ROC curve
colAUC(p,test$Class, plotROC = TRUE)
#### ROC CURVE
library(caTools)
# Make ROC curve
colAUC(p,test$Class, plotROC = TRUE)
#### AUC
# Create trainControl object: myControl
myControl <- trainControl(
method = "cv",
number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE
)
# Train glm with custom trainControl: model
model <- train(Class ~ ., data = Sonar, method = "glm", trControl = myControl)
# Print model to console
model
setwd("E:/Datacamp/R/Machine Learning Toolbox")
wine <- readRDS('wine_100.RDS')
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 1,
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 1,
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
model
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 3,
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
model
# Plot model
plot(model)
?train
?ranger
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 3,
data = wine, method = "ranger", splitrule = 'extratrees'
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 3,
data = wine, method = "ranger", splitrule = 'extratrees',
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 3,
data = wine, method = "ranger", splitrule = 'extratrees',
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
model
# Plot model
plot(model)
?train
?ranger
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 10, #RF has primary tuning parameter: mtry: number of randomly selected preidctors
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
model
# Plot model
plot(model)
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 3, #RF has primary tuning parameter: mtry: number of randomly selected preidctors
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
model
# Plot model
plot(model)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7)),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7)),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Fit random forest: model
model <- train(
quality ~ .,
tuneLength = 3, #RF has primary tuning parameter: mtry: number of randomly selected preidctors
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
model
# Plot model
plot(model)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7)),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule = None),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule = c('variance', 'extratrees')),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7)),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
model
# Plot model
plot(model)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7)),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
?train
modelLookup('ranger')
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule, nin.node.size),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule, nin.node.size),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule = 0, nin.node.size),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule = 0, nin.node.size = 0),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule = 0, min.node.size = 0),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule = "variance", min.node.size = 0),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
#### CUSTOM TUNING
# Fit random forest: model
model <- train(
quality ~ .,
tuneGrid = data.frame(mtry = c(2,3,7), splitrule = "variance", min.node.size = 5),
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
model
# Plot model
plot(model)
#### TUNE GLMNET
# Create custom trainControl: myControl
myControl <- trainControl(
method = "cv", number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE
)
# Fit glmnet model: model
model <- train(
y ~ ., data = overfit,
method = "glmnet",
trControl = myControl
)
overfit = read.csv('overfit.csv')
# Fit glmnet model: model
model <- train(
y ~ ., data = overfit,
method = "glmnet",
trControl = myControl
)
# Print model to console
model
plot(model)
# Print maximum ROC statistic
max(model[["results"]])
model[['result']]
model[['results']]
# Print maximum ROC statistic
max(model[["results"]][['ROC']])
# Train glmnet with custom trainControl and tuning: model
model <- train(
y ~ ., overfit,
tuneGrid = expand.grid(
alpha = 0:1,
lambda = seq(0.0001, 1, length = 20)),
method = "glmnet",
trControl = myControl
)
# Print model to console
model
plot(model)
setwd("E:/Datacamp/R/Machine Learning Toolbox")
load('BreastCancer.RData')
View(breast_cancer_x)
#### MEDIAN IMPUTATION
# Apply median imputation: model
model <- train(
x = breast_cancer_x, y = breast_cancer_y,
method = "glm",
trControl = myControl,
preProcess = "medianImpute"
)
# Create custom trainControl: myControl
myControl <- trainControl(
method = "cv", number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE
)
#### MEDIAN IMPUTATION
# Apply median imputation: model
model <- train(
x = breast_cancer_x, y = breast_cancer_y,
method = "glm",
trControl = myControl,
preProcess = "medianImpute"
)
# Print model to console
model
#### KNN IMPUTATION (for missing not at random)
# Apply KNN imputation: model2
model2 <- train(
x = breast_cancer_x, y = breast_cancer_y,
method = "glm",
trControl = myControl,
preProcess = "knnImpute"
)
# Print model to console
model2
#### KNN IMPUTATION (for missing not at random)
# Apply KNN imputation: model2
model2 <- train(
x = breast_cancer_x, y = breast_cancer_y,
method = "glm",
trControl = myControl,
preProcess = "knnImpute"
)
install.packages("RANN")
#### KNN IMPUTATION (for missing not at random)
# Apply KNN imputation: model2
model2 <- train(
x = breast_cancer_x, y = breast_cancer_y,
method = "glm",
trControl = myControl,
preProcess = "knnImpute"
)
# Print model to console
model2
#### ADD STANDARDIZATION TO PREPROCESS
# Fit glm with median imputation and standardization: model2
model2 <- train(
x = breast_cancer_x, y = breast_cancer_y,
method = "glm",
trControl = myControl,
preProcess = c("medianImpute", "center", "scale")
)
# Print model2
model2
#### ADD STANDARDIZATION TO PREPROCESS
# Fit glm with median imputation and standardization: model2
model2 <- train(
x = breast_cancer_x, y = breast_cancer_y,
method = "glm",
trControl = myControl,
preProcess = c("medianImpute", "center", "scale", 'pca')
)
# Print model2
model2
load('BloodBrain.RData')
# REMOVE NEAR ZERO VARIANCE PREDICTORS
# Identify near zero variance predictors: remove_cols
remove_cols <- nearZeroVar(bloodbrain_x, names = TRUE,
freqCut = 2, uniqueCut = 20)
# Get all column names from bloodbrain_x: all_cols
all_cols <- names(bloodbrain_x)
# Remove from data: bloodbrain_x_small
bloodbrain_x_small <- bloodbrain_x[ , setdiff(all_cols, remove_cols)]
?nearZeroVar
# Method2: Remove near zero value in preprocessing
# Fit model on reduced data: model
model <- train(x = bloodbrain_x, y = bloodbrain_y, method = "glm",
preProcess = c('nzv'))
# Print model to console
model
?train
#### ALTERNATIVE TO NEARZEROVAR(): PCA
# more preferable because it does not throw out all data
# many different low variance predictors may end up combined into one high variance PCA variable
# Fit glm model using PCA: model
model <- train(
x = bloodbrain_x, y = bloodbrain_y,
method = "glm", preProcess = "pca"
)
# Print model to console
model
setwd("E:/Datacamp/R/Machine Learning Toolbox")
# Create custom indices: myFolds
myFolds <- createFolds(churn_y, k = 5)
load('churn.RData')
library(caret)
# Create custom indices: myFolds
myFolds <- createFolds(churn_y, k = 5)
# Create reusable trainControl object: myControl
myControl <- trainControl(
summaryFunction = twoClassSummary,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
savePredictions = TRUE,
index = myFolds
)
#### FIT GLMNET
# Fit glmnet model: model_glmnet
model_glmnet <- train(
x = churn_x, y = churn_y,
metric = "ROC",
method = "glmnet",
trControl = myControl
)
#### FIT RANDOMFOREST
# Fit random forest: model_rf
model_rf <- train(
x = churn_x, y = churn_y,
metric = "ROC",
method = "ranger",
trControl = myControl
)
#### CREATE A RESAMPLE OBJECT
# Create model_list
model_list <- list(item1 = model_glmnet, item2 = model_rf)
# Pass model_list to resamples(): resamples
resamples <- resamples(model_list)
# Summarize the results
summary(resamples)
# Create bwplot
bwplot(resamples, metric = "ROC")
# Create xyplot
xyplot(resamples, metric = "ROC")
#### ENSEMBLE MODEL
# Create ensemble model: stack
stack <- caretStack(model_list, method = "glm")
install.packages("caretEnsemble")
#### ENSEMBLE MODEL
library(caretEnsemble)
# Create ensemble model: stack
stack <- caretStack(model_list, method = "glm")
# Look at summary
summary(stack)
# Create ensemble model: stack
stack <- caretStack(model_list, method = "glm")
model_list
# Create dotplot
dotPlot(resamples, metric = 'ROC')
# Create xyplot
xyplot(resamples, metric = "ROC")
# Create dotplot
dotplot(resamples, metric = 'ROC')
# Density plot
densityplot(resamples, metric = 'ROC')
#### ENSEMBLE MODEL
library(caretEnsemble)
# Create ensemble model: stack
stack <- caretStack(model_list, method = "glm")
?caretStack
# Creat caretlist
models <- caretList(x = churn_x, y = churn_y,
trControl = myControl,
methodList = c('glmnet', 'ranger'))
# Create ensemble model: stack
stack <- caretStack(models, method = "glm")
# Look at summary
summary(stack)

?ncbirths
install.packages("openintro")
library(openintro)
ncbirths
#### SCATTER PLOT
# Scatterplot of weight vs. weeks
ggplot(ncbirths, aes(weeks, weight)) +
geom_point()
library(ggplot2)
#### SCATTER PLOT
# Scatterplot of weight vs. weeks
ggplot(ncbirths, aes(weeks, weight)) +
geom_point()
#### BOXPLOT
# Boxplot of weight vs. weeks
ggplot(data = ncbirths,
aes(x = cut(weeks, breaks = 5), y = weight)) +
geom_boxplot()
# Mammals scatterplot
ggplot(mammals, aes(BodyWt, BrainWt)) +
geom_point()
# Baseball player scatterplot
ggplot(mlbBat10, aes(OBP, SLG)) +
geom_point()
# Body dimensions scatterplot
ggplot(bdims, aes(hgt, wgt, col = factor(sex))) +
geom_point()
# Smoking scatterplot
ggplot(smoking, aes(age, amtWeekdays)) +
geom_point()
#### SCATTER PLOT
# Scatterplot of weight vs. weeks
ggplot(ncbirths, aes(weeks, weight)) +
geom_point()
# Mammals scatterplot
ggplot(mammals, aes(BodyWt, BrainWt)) +
geom_point()
# Scatterplot with coord_trans()
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
geom_point() +
coord_trans(x = "log10", y = "log10")
# Scatterplot with scale_x_log10() and scale_y_log10()
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
geom_point() +
scale_x_log10() + scale_y_log10()
#### OUTLIER
# Scatterplot of SLG vs. OBP
mlbBat10 %>%
filter(AB >= 200) %>%
ggplot(aes(x = OBP, y = SLG)) +
geom_point()
# Identify the outlying player
mlbBat10 %>%
filter(AB >= 200, OBP < 0.2)
library(dpylr)
library(dplyr)
#### OUTLIER
# Scatterplot of SLG vs. OBP
mlbBat10 %>%
filter(AB >= 200) %>%
ggplot(aes(x = OBP, y = SLG)) +
geom_point()
# Identify the outlying player
mlbBat10 %>%
filter(AB >= 200, OBP < 0.2)
#### OUTLIER
# Original scatter plot
ggplot(mlbBat10, aes(x = OBP, y = SLG)) +
geom_point()
# Scatterplot of SLG vs. OBP
mlbBat10 %>%
filter(AB >= 200) %>%
ggplot(aes(x = OBP, y = SLG)) +
geom_point()
#### CORRELATION
# Compute correlation
ncbirths %>%
summarize(N = n(), r = cor(mage, weight))
# Compute correlation for all non-missing pairs
ncbirths %>%
summarize(N = n(), r = cor(weeks, weight, use = "pairwise.complete.obs"))
#### ANSCOMBE DATASET
# Compute properties of Anscombe
Anscombe %>%
group_by(set) %>%
summarize(N = n(), mean(x), sd(x), mean(y), sd(y), cor(x,y))
# Scatterplot with regression line
ggplot(data = bdims, aes(x = hgt, y = wgt)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
df <- c([2,3])
df <- c(2, 3)
df <- c((2, 3), (4, 5))
df <- c(2, 4)
?data.frame
df <- data.frame(x = c(2, 3), y = c(4,5))
View(df)
# Replace the 1:ncol(df) sequence
for (i in seq_along(df)) {
print(median(df[[i]]))
}
# Change the value of df
df <- data.frame()
# Repeat for loop to verify there is no error
for (i in seq_along(df)) {
print(median(df[[i]]))
}
seq_along(df)
# Repeat for loop to verify there is no error
for (i in seq_along(df)) {
print(median(df[[i]]))
}
for (i in 1:ncol(df)) {
print(median(df[[i]]))
}
for (i in seq_along(df)) {
print(median(df[[i]]))
}
ncol(df)
df <- data.frame(x = c(2, 3), y = c(4,5))
# Replace the 1:ncol(df) sequence
for (i in seq_along(df)) {
print(median(df[[i]]))
}
for (i in 1:ncol(df)) {
print(median(df[[i]]))
}
# Change the value of df
df <- data.frame()
for (i in 1:ncol(df)) {
print(median(df[[i]]))
}
# Repeat for loop to verify there is no error
for (i in seq_along(df)) {
print(median(df[[i]]))
}
df <- data.frame(x = c(2, 3), y = c(4,5))
#### STORE OUTPUT, NOT SHOW => INCREASE EFFICIENCY
# Create new double vector: output
output <- vector("double", ncol(df))
# Alter the loop
for (i in seq_along(df)) {
# Change code to store result in output
output[[i]] <- median(df[[i]])
}
# Print output
output
df[1]
df[[1]]
#### START WITH SIMPLE PROBLEM
# Define example vectors x and y
x <- c( 1, 2, NA, 3, NA)
y <- c(NA, 3, NA, 3,  4)
# Count how many elements are missing in both x and y
sum(is.na(x) & is.na(y))
#### REWRITE SNIPPET AS A FUNCTION
# Define example vectors x and y
x <- c( 1, 2, NA, 3, NA)
y <- c(NA, 3, NA, 3,  4)
# Count how many elements are missing in both x and y
sum(is.na(x) & is.na(y))
# Turn this snippet into a function: both_na()
both_na <- function(x, y) {
sum(is.na(x) & is.na(y))
}
#### START WITH SIMPLE PROBLEM
# Define the dataframe df
df <- data.frame(
a = rnorm(10),
b = rnorm(10),
c = rnorm(10),
d = rnorm(10)
)
View(df)
# Rescelae the a column in df to a 0-1 range
(df$a - min(df$a, na.rm = TRUE)) /
(max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))
# Assign a column to a temporary variable
x <- df$a
# Rewrite the code with temporary variable
( x - min( x , na.rm = TRUE)) /
(max( x , na.rm = TRUE) - min( x , na.rm = TRUE))
#### TURN INTO A FUNCTION
# Define the function
rescale01 <- function(x){
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
# Run the function
rescale01(df$a)
# Define the dataframe df
df <- data.frame(
a = rnorm(10),
b = rnorm(10),
c = rnorm(10),
d = rnorm(10)
)
#### THE ... ARGUMENT TO THE MAP FUNCTION
# Find the mean of each column
map_dbl(df, mean)
library(purrr)
#### THE ... ARGUMENT TO THE MAP FUNCTION
# Find the mean of each column
map_dbl(df, mean)
# Find the mean of each column, excluding missing values
map_dbl(df, mean, na.rm = TRUE)
# Find the 5th percentile of each column, excluding missing values
map_dbl(df, quantile, probs = 0.05, na.rm = TRUE)
#### PICK THE RIGHT MAP FUNCTION
# Find the columns that are numeric
map_lgl(df, is.numeric)
# Find the type of each column
map_chr(df, typeof)
# Find a summary of each column
map(df3, summary)
# Find a summary of each column
map(df, summary)
map(df, is.numeric)
#### SHORTCUTS WITHIN A FUNCTION
# Rewrite to use the formula shortcut instead
map(df, ~ lm(a ~ b, data = .))
df
lm(a ~ b, data = df)
?split
#### SHORTCUTS WITHIN A FUNCTION
# Split dataframe mtcars based on cyl
cyl <- split(mtcars, mtcars$cyl)
# Rewrite to use the formula shortcut instead
map(df, ~ lm(a ~ b, data = .))
# Rewrite to use the formula shortcut instead
map(cyl, ~ lm(mpg ~ wt, data = .))
#### SHORTCUT USING A STRING
# Save the result from the previous exercise to the variable models
models <- map(cyl, ~ lm(mpg ~ wt, data = .))
# Use map and coef to get the coefficients for each model: coefs
coefs <- map(models, coef)
# Use string shortcut to extract the wt coefficient
map(coefs, "wt")
# Another way is to use numeric vector
map_dbl(coefs, 2)
# Create safe_readLines() by passing readLines() to safely()
safe_readLines <- safely(readLines)
# Call safe_readLines() on "http://example.org"
safe_readLines("http://example.org")
# Call safe_readLines() on "http://asdfasdasdkfjlda"
safe_readLines("http://asdfasdasdkfjlda")
#### CREATE SAFE FUNCTION
# safely()  takes a function as an argument and it returns a function as its output.
# The function that is returned is modified so it never throws an error
# always returns a list with 2 elements: result, error
# Define list of URLs
urls <- list(
example = "http://example.org",
rproj = "http://www.r-project.org",
asdf = "http://asdfasdasdkfjlda"
)
# Create safe_readLines() by passing readLines() to safely()
safe_readLines <- safely(readLines)
# Use the safe_readLines() function with map(): html
html <- map(urls, safe_readLines)
# Call str() on html
str(html)
# Extract the result from one of the successful elements
html[["example"]][["result"]]
# Extract the error from the element that was unsuccessful
html[["asdf"]][["error"]]
# Use the safe_readLines() function with map(): html
html <- map(urls, safe_readLines)
View(html)
# Extract the result from one of the successful elements
html[["example"]][["result"]]
# Extract the error from the element that was unsuccessful
html[["asdf"]][["error"]]
View(html)
# Examine the structure of transpose(html)
str(transpose(html))
# Extract the results: res
res <- transpose(html)[["result"]]
# Extract the errors: errs
errs <- transpose(html)[["error"]]
View(res)
res
errs
# Create a logical vector is_ok
is_ok <- map_lgl(errs, is_null)
# Extract the successful results
res[is_ok]
# Extract the input from the unsuccessful results
urls[!is_ok]
#### MAP OVER 2 ARGUMENTS
# Initialize n
n <- list(5, 10, 20)
# Create a list mu containing the values: 1, 5, and 10
mu <- list(1, 5, 10)
# Edit to call map2() on n and mu with rnorm() to simulate three samples
map2(n, mu, rnorm)
#### MAP OVER MORE THAN 2 ARGUMENTS
# Initialize n and mu
n <- list(5, 10, 20)
mu <- list(1, 5, 10)
# Create a sd list with the values: 0.1, 1 and 0.1
sd <- list(0.1, 1, 0.1)
# Edit this call to pmap() to iterate over the sd list as well
pmap(list(n, mu, sd), rnorm)
#### MAP OVER FUNCTIONS AND ARGUMENTS
# Define list of functions
f <- list("rnorm", "runif", "rexp")
# Parameter list for rnorm()
rnorm_params <- list(mean = 10)
View(rnorm_params)
# Add a min element with value 0 and max element with value 5
runif_params <- list(min = 0, max = 5)
# Add a rate element with value 5
rexp_params <- list(rate = 5)
# Define params for each function
params <- list(
rnorm_params,
runif_params,
rexp_params
)
View(params)
# Call invoke_map() on f supplying params as the second argument
invoke_map(f, params, n = 5)
#### MAP WITH SIDE EFFECTS: WALK
# Side effects = printing, plotting or saving.
# Define list of functions
f <- list(Normal = "rnorm", Uniform = "runif", Exp = "rexp")
# Define params
params <- list(
Normal = list(mean = 10),
Uniform = list(min = 0, max = 5),
Exp = list(rate = 5)
)
# Assign the simulated samples to sims
sims <- invoke_map(f, params, n = 50)
# Use walk() to make a histogram of each element in sims
walk(sims, hist)
# Replace "Sturges" with reasonable breaks for each sample
breaks_list <- list(
Normal = seq(6, 16, 0.5),
Uniform = seq(0, 5, 0.25),
Exp = seq(0, 1.5, 0.1)
)
# Use walk2() to make histograms with the right breaks
walk2(sims, breaks_list, hist)
# Use map() to iterate find_breaks() over sims: nice_breaks
nice_breaks <- map(sims, find_breaks)
# Generate reasonable breaks based on actual values in samples
# Turn this snippet into find_breaks()
find_breaks <- function(x) {
rng <- range(x, na.rm = TRUE)
seq(rng[1], rng[2], length.out = 30)
}
# Use map() to iterate find_breaks() over sims: nice_breaks
nice_breaks <- map(sims, find_breaks)
# Use nice_breaks as the second argument to walk2()
walk2(sims, nice_breaks, hist)
#### WALK WITH MANY ARGUMENTS
# Increase sample size to 1000
sims <- invoke_map(f, params, n = 1000)
# Compute nice_breaks (don't change this)
nice_breaks <- map(sims, find_breaks)
# Create a vector nice_titles
nice_titles <- list("Normal(10, 1)", "Uniform(0, 5)", "Exp(5)")
# Use pwalk() instead of walk2()
pwalk(list(x = sims, breaks = nice_breaks, main = nice_titles), hist, xlab = "")
#### WALK WITH PIPE
# Pipe this along to map(), using summary() as .f
sims %>%
walk(hist) %>%
map(summary)
# Structure
str(sims %>% walk(hist))
# Define troublesome x and y
x <- c(NA, NA, NA)
y <- c( 1, NA, NA, NA)
both_na <- function(x, y) {
# Add stopifnot() to check length of x and y
stopifnot(length(x) == length(y))
sum(is.na(x) & is.na(y))
}
# Call both_na() on x and y
both_na(x, y)
# Initial function to finds number of entries where x and y both missing
both_na <- function(x, y) {
sum(is.na(x) & is.na(y))
}
# Call both_na() on x and y
both_na(x, y)
# Alter function to use informative error
both_na <- function(x, y) {
# Replace condition with logical
if (length(x) != length(y)) {
# Replace "Error" with better message
stop("x and y must have the same length", call. = FALSE)
}
sum(is.na(x) & is.na(y))
}
# Call both_na()
both_na(x, y)
df <- data.frame(
a = 1L,
b = 1.5,
y = Sys.time(),
z = ordered(1)
)
View(df)
# sapply calls
A <- sapply(df[1:4], class)
B <- sapply(df[3:4], class)
C <- sapply(df[1:2], class)
# Demonstrate type inconsistency
str(A)
str(B)
str(C)
# Use map() to define X, Y and Z
X <- map(df[1:4], class)
Y <- map(df[3:4], class)
Z <- map(df[1:2], class)
# Use str() to check type consistency
str(X)
str(Y)
str(Z)
#### A TYPE CONSISTENT SOLUTION
col_classes <- function(df) {
# Assign list output to class_list
class_list <- map(df, class)
# Use map_chr() to extract first element in class_list
map_chr(class_list, 1)
}
# Check that our new function is type consistent
df %>% col_classes() %>% str()
df[3:4] %>% col_classes() %>% str()
col_classes <- function(df) {
class_list <- map(df, class)
# Add a check that no element of class_list has length > 1
if (any(map_dbl(class_list, length) > 1)) {
stop("Some columns have more than one class", call. = FALSE)
}
# Use flatten_chr() to return a character vector
flatten_chr(class_list)
}
# Check that our new function is type consistent
df %>% col_classes() %>% str()
df[3:4] %>% col_classes() %>% str()
df[1:2] %>% col_classes() %>% str()
big_x <- function(df, threshold) {
dplyr::filter(df, x > threshold)
}
# Use big_x() to find rows in mt_cars where disp > 400
big_x(disp, 400)
big_x <- function(df, threshold) {
dplyr::filter(df, disp > threshold)
}
# Use big_x() to find rows in mt_cars where disp > 400
big_x(disp, 400)
# Use big_x() to find rows in mt_cars where disp > 400
big_x(mtcars, 400)
mtcars_sub = mtcars
# Remove the disp column from mtcars_sub
mtcars_sub$x <- NULL
# Remove the disp column from mtcars_sub
mtcars_sub$disp <- NULL
# Use big_x() to find rows in mtcars_sub where disp > 400
big_x(mtcars_sub, 400)
# Create a threshold column with value 100
mtcars_sub$threshold <- 100
# Use big_x() to find rows in diamonds_sub where x > 7
big_x(mtcars_sub, 400)
#### WHAT TO DO
big_x <- function(df, threshold) {
# Write a check for disp not being in df
if(!"disp" %in% names(df)) {
stop("df must contain variable called disp", call. = FALSE)
}
# Write a check for threshold being in df
if("threshold" %in% names(df)) {
stop("df must not contain variable called threshold", call. = FALSE)
}
dplyr::filter(df, x > threshold)
}
setwd("E:/Datacamp/R/Writing Functions")
#### HIDDEN INDEPENDENCE
setwd("E:/Datacamp/R/Writing Functions")
# Read in the swimming_pools.csv to pools
pools <- read.csv("swimming_pools.csv")
# Examine the structure of pools
str(pools)
# Change the global stringsAsFactor option to FALSE
options(stringsAsFactors = FALSE)
# Read in the swimming_pools.csv to pools2
pools2 <- read.csv("swimming_pools.csv")
# Examine the structure of pools2
str(pools2)
# Change the global stringsAsFactor option to FALSE
getOption("stringsAsFactors")
